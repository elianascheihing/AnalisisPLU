{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Tópicos: una revisión\n",
    "\n",
    "- Probabilistic topic models, Blei, Communication ACM, 2012: https://doi.org/10.1145/2133806.2133826\n",
    "\n",
    "\n",
    "- Latent Dirichlet Allocation, Blei,Ng y Jordan, The Journal of Machine Learning Research, 2003, https://dl.acm.org/doi/10.5555/944919.944937\n",
    "\n",
    "\n",
    "- Dynamic topic models, Blei y Lafferty, 2006, https://dl.acm.org/doi/10.1145/1143844.1143859\n",
    "\n",
    "\n",
    "- Efficient Estimation of Word Representations in Vector Space, Mikolov et al, ICLR 2013, https://arxiv.org/abs/1301.3781\n",
    "\n",
    "\n",
    "- Attention word embedding, Sonkar et al 2020, https://arxiv.org/abs/2006.00988\n",
    "\n",
    "\n",
    "- Topic Modeling in Embedding Spaces, Dieng, Ruiz y Blei, 2019, https://arxiv.org/abs/1907.04907\n",
    "\n",
    "\n",
    "- The Dynamic Embedded Topic Model, Dieng, Ruiz y Blei, 2019-21, http://arxiv-export-lb.library.cornell.edu/pdf/1907.05545v2\n",
    "\n",
    "\n",
    "- Global Surveillance of COVID-19 by mining news media using a multi-source dynamic embedded topic model,Li et al,  BCB 2020, https://dl.acm.org/doi/10.1145/3388440.3412418\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelamiento probabilista de tópicos busca  descubrir la distribución de los tópicos principales en una colección de documentos no estructurados:\n",
    "\n",
    "<img src=\"modelo.png\" width=500 height=200>\n",
    "\n",
    "\n",
    "\n",
    "- Algoritmos no tienen informaciones a priori (no consideran etiquetas/labels/clases)\n",
    "\n",
    "- Algoritmo clásico: Latent Dirichlet Allocation, modelo de \"bolsa de palabras\" (BOW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usualmente, los tópicos se caracterizan por sus palabras mas frecuentes y cada documento por su distribución de tópicos. Ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"f1.jpg\" width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"f2.jpg\" width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"f3.png\" width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datos de entrada en un modelo probabilista de tópicos:**\n",
    "\n",
    "- una representación vectorial de un conjunto de documentos: \n",
    "       - Bag-of-Words (ej: tf-idf)\n",
    "       - Word Embeddings (ej: word2vec) \n",
    "\n",
    "- un diccionario (el vocabulario)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "Dado un número de tópicos definido por el analista ($K$), el modelo permite asociar una distribución de tópicos $\\theta_d$ a cada documento $d$ y al mismo tiempo, la distribución de palabras $\\beta_k$ en cada tópico $k$.\n",
    "\n",
    "Así, cada documento queda definido (es generado por)  por una distribución de tópicos, que es una probabilidad discreta: \n",
    "\n",
    "$$\\theta_d \\sim Dirichlet(\\alpha_{1:K})\\qquad d=1,\\cdots,D$$\n",
    "\n",
    "$$P(\\theta_d=(\\theta_{d,1},\\cdots,\\theta_{d,K})) \\propto \\theta_{d,1}^{\\alpha_1}\\theta_{d,2}^{\\alpha_2}\\cdots \\theta_{d,K}^{\\alpha_K}\\qquad\\qquad\n",
    "\\sum_{k=1}^K \\theta_{d,k} = 1\\qquad \\theta_{d,k} \\geq 0 \\,\\,\\forall k$$\n",
    "\n",
    "Y cada tópico queda definido (es generado por) por una probabilidad discreta sobre el vocabulario del Corpus (de dimensión $N$) que cumple:\n",
    "\n",
    "$$\\beta_k \\sim Dirichlet(\\eta_{1:N})\\qquad k=1,\\cdots,K$$\n",
    "\n",
    "$$P(\\beta_k=(\\beta_{k,1},\\cdots,\\beta_{k,N})) \\propto \\beta_{k,1}^{\\eta_1}\\beta_{k,2}^{\\eta_2}\\cdots \\beta_{k,N}^{\\eta_N}\\qquad\\qquad\n",
    "\\sum_{i=1}^N \\beta_{k,i} = 1\\qquad \\beta_{k,i}\\geq 0 \\,\\,\\forall i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso generativo consiste en lo siguiente: \n",
    "\n",
    "1) Dados valores para las distribuciones que definen los tópicos: $\\beta_1,\\cdots \\beta_K$ y los valores para las distribuciones de tópicos en los documentos: $\\theta_1,\\cdots \\theta_D$ \n",
    "\n",
    "2) La n-ésima palabra $w_{d,n}$ en el documento $d$, se genera con el siguiente proceso:\n",
    "\n",
    "Sea $z_{d,n}$ la v.a. que representa el tópico asociado a la n-ésima palabra $w_{d,n}$, el valor que toma $z_{d,n}$ se genera de acuerdo a la distribución:\n",
    "$$z_{d,n}\\sim Mult(\\theta_d)$$\n",
    "\n",
    "supongamos que el tópico asignado sea $z_{d,n} =k$, entonces la palabra $w_{d,n}=w$ se genera de acuerdo a la distribución\n",
    "\n",
    "$$w_{d,n} \\sim Mult(\\beta_k) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"simplex.png\" width=600 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proceso generador de las palabras en cada documento**\n",
    "\n",
    "Sean \n",
    "-  $\\beta_{1:K}$ conjunto de $K$ tópicos \n",
    "-  $\\theta_d$ distribución de tópicos en el documento $d$\n",
    "-  $z_{d,n}$ tópico asignado a palabra $n-$ésima en $d$\n",
    "-  $w_{d,n}$ la $n-$ésima palabra observada en $d$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$$ p(\\beta_{1:K},\\theta_{1:D},z_{1:N},w_{1:N}) = \\prod_{k=1}^K p(\\beta_k) \\prod_{d=1}^D p(\\theta_d)\\left(\\prod_{n=1}^{N_d} p(z_{d,n} \\mid \\theta_d) p(w_{d,n}\\mid \\beta_{1:K},z_{d,n})\\right)$$\n",
    "dónde $$\\theta_d \\sim Dirichlet(\\alpha_{1:K})  \\qquad \\beta_k \\sim Dirichlet(\\eta_{1:N})$$\n",
    "    y $$z_{d,n} \\mid \\theta_d \\sim Mult(\\theta_d) \\qquad  w_{d,n} \\mid z_{d,n},\\beta_{1:K} \\sim Mult(\\phi_{z_{d,n}})$$\n",
    "    \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"f4.png\" width=600 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo de la distribución a posteriori\n",
    "Para descubrir la estructura oculta del modelo, se debe calcular la distribución a posteriori de los parámetros del modelo, esto es:\n",
    "\n",
    "$$p(\\beta_{1:K},\\theta_{1:D},z_{1:N}\\mid w_{1:N}) = \\frac{p(\\beta_{1:K},\\theta_{1:D},z_{1:N},w_{1:N})}{p(w_{1:N})}$$\n",
    "\n",
    "por la propiedad de natural conjugada de la distribución de Dirichlet con respecto a la distribución Multinomial, el numerador de esta expresión es fácilmente calculable. No es el caso del denominador, que considera todas las posibles estructuras de tópicos, lo que crece exponencialmente con el número de tópicos, tamaño del vocabulario y número de documentos.\n",
    "\n",
    "Se consideran dos formas de aproximar la distribución a posteriori:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) mediante técnicas de muestreo**\n",
    "que se basan en construir una cadena de Markov, cuya distribución en el equilibrio es la distribución a posteriori. De esta forma, se obtiene una muestra de la cadena de markov, que se asume (en el equilibrio) proviene de la distribución buscada. El caso mas conocido es Gibbs Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) mediante calculo variacional**\n",
    "que considera una familia de distribuciones parametrizadas que se acercan a la   la distribución a posteriori, y el problema a resolver, es encontrar los parámetros que minimizan la distancia a la distribución buscada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de la Calidad de los Tópicos\n",
    "\n",
    "**i) Perplejidad:**\n",
    "Esta medida se considera intrínseca y proviene del modelamiento probabilista realizado. Su expresión es:\n",
    "\n",
    "$$perplejidad(D_{test}) = exp\\left\\{-\\frac{\\prod_{d=1}^D log(p(w_d))}{\\sum_{d=1}^D N_d}\\right\\}$$\n",
    "\n",
    "Para utilizarla se debe considerar un conjunto de entrenamiento y otro de test. De manera que la perplejidad se interpreta como una medida de robustez de la estructura de tópicos definida. \n",
    "Es inversamente proporcional a la verosimilitud de los nuevos datos. A menor valor de perplejidad mas robusto el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Métricas de Coherencia:**\n",
    "\n",
    "Probabilidad condicional que dos palabras aparezcan juntas \n",
    "(idealmente en un dataset de referencia)\n",
    "\n",
    "Existen muchas métricas de coherencia. Como por ejemplo:\n",
    "\n",
    "**Coherencia UMass**, propuesta por por Minmo et al(2011):\n",
    "\n",
    "$$C_{umass}= \\frac{2}{N(N-1)}\\sum_{i=2}^N \\sum_{j=1}^{i-1} log \\frac{p(w_i,w_j)}{p(w_j)}$$\n",
    "\n",
    "Para cada tópico se calcula su coherencia considerando las N top words del mismo. En este caso, se utilizan las probabilidades calculadas a partir del corpus en estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos  de tópicos dinámicos\n",
    "\n",
    "El objetivo de los modelos de tópicos dinámicos  es estudiar la deriva de los conceptos al interior de los tópicos.\n",
    "En <a href=\"https://mimno.infosci.cornell.edu/info6150/readings/dynamic_topic_models.pdf\"> Blei y Lafferty (2006)</a> se estudia el caso de las publicaciones científicas en la revista Science. Se consideraron 30,000 articulos, 250 en cada uno de los 120 años desde 1881 y 1999. Y se busca predecir los tópicos (la variación al interior de cada uno) en el año siguiente (2000).\n",
    "\n",
    "El tipo de resultados que obtienen es el siguiente:\n",
    "\n",
    "#### Tópico Física Atómica\n",
    "\n",
    "<img src=\"topicos1.png\" width=800 height=200>\n",
    "<img src=\"topicos2.png\" width=800 height=200>\n",
    "\n",
    "\n",
    "#### Tópico Neurociencia\n",
    "\n",
    "<img src=\"topicos3.png\" width=800 height=200>\n",
    "<img src=\"topicos4.png\" width=800 height=200>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelar la evolución temporal de tópicos $k, k=1,\\cdots K$ se definen franjas (slices) de tiempo t y se reemplazan las distribuciones a priori Dirichlet  por **Logistic Normal**  en dos pasos:\n",
    "\n",
    "$$\\beta_{t,k} \\mid \\beta_{t-1,k} \\sim {\\cal{N}}(\\beta_{t-1,k}, \\sigma^2 I_{NxN}), \\qquad k=1,\\cdots K$$\n",
    "\n",
    "que se mapea en el simplex mediante la transformación logística, también denominada *softmax*:\n",
    "\n",
    "$$\\pi(\\beta_{t,k})_w = \\frac{exp(\\beta_{t,k,w})}{\\sum_{w \\in W} exp(\\beta_{t,k,w})}\\qquad k=1,\\cdots K$$\n",
    "\n",
    "\n",
    "donde $|W| =N$.\n",
    "\n",
    "Y lo mismo se realiza para la evolución temporal de la distribución de tópicos:\n",
    "\n",
    "$$\\alpha_t \\mid \\alpha_{t-1} \\sim {\\cal{N}}(\\alpha_{t-1}, \\delta^2 I_{KxK})$$\n",
    "\n",
    "\n",
    "De esta manera se propone el siguiente proceso generativo secuencial:\n",
    "\n",
    "1. Generar tópicos \n",
    "\n",
    "$$\\beta_{t,k} \\mid \\beta_{t-1,k} \\sim {\\cal{N}}(\\beta_{t-1}, \\sigma^2 I_{NxN})$$\n",
    "\n",
    "y las respectivas funciones logísticas: \n",
    "\n",
    "$$\\phi_{t,k} = \\pi(\\beta_{t,k})_w = \\frac{exp(\\beta_{t,k,w})}{\\sum_{w \\in W} exp(\\beta_{t,k,w})}\\qquad k=1,\\cdots K$$\n",
    "\n",
    "2. Generar\n",
    "\n",
    "$$\\alpha_t \\mid \\alpha_{t-1} \\sim {\\cal{N}}(\\alpha_{t-1}, \\delta^2 I_{KxK})$$\n",
    "\n",
    "\n",
    "3. Por cada documento:\n",
    "\n",
    "    (a) Generar $\\theta_d \\sim {\\cal{N}}(\\alpha_{t_d}, \\delta^2 I_{KxK}))$\n",
    "    \n",
    "    (b) Por cada palabra $n$ en el documento $d$, generar\n",
    "        \n",
    "$$z_{d,n} \\sim Mult(\\pi(\\theta_d)) \\qquad \\text{y} \\qquad w_{t_d,n} \\sim Mult(\\phi(t_d,z))$$\n",
    "\n",
    "\n",
    "<img src=\"dtm.png\" width=600 height=400>         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, la expresión de la distribución a posteriori \n",
    "\n",
    "$$p(\\phi_{1:T},\\theta_{1:D},z_{1:N}\\mid w_{1:N}) = \\frac{p(\\phi_{1:T},\\theta_{1:D},z_{1:N}, w_{1:N}) }{p(w_{1:N})}$$\n",
    "\n",
    "no es fácilmente calculable, incluso el numerador, debido a que la Normal Logistic no es natural conjugada de la distribución Multinomial.\n",
    "\n",
    "Es por ello que se recurre a métodos variacionales para aproximar la distribución a posteriori por una distribución cuyos parámetros se ajustan para que tenga mínima distancia de KL con la distribución a posteriori.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la formulación teórica, los parámetros que se requiere definir al realizar las aproximaciones son:\n",
    "\n",
    "- $\\sigma^2$ variance chain: es el parámetro que define la magnitud del ruido gaussiano que modela la variación temporal de los tópicos\n",
    "\n",
    "- $\\alpha =(\\alpha_1,\\cdots ,\\alpha_K)$ es el vector de parámetros de la distribución Dirichlet, que modela la variabilidad de tópicos en cada documento. Valores menores que uno y cercanos a 0 representan poca variabilidad entre tópicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitaciones del Modelo de Tópicos Dinámicos**\n",
    "\n",
    "- el número de tópicos no cambia en el tiempo\n",
    "- no interpreta automáticamente el significado de los tópicos (se requiere la interpretación del humano, no elimina subjetividad)\n",
    "- modela tópicos según nuestros a priori (Número de tópicos, variance chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de Word Embedding \n",
    "\n",
    "Los métodos de word embedding se construyen en base a grandes corpus de documentos que permiten entrenar redes neuronales artificiales con diversas arquitecturas, de tal forma que se consideran **los pesos de la capa interna de la red** como la representación de cada palabra.\n",
    "\n",
    "**Continuous bag-of-words (CBOW):** En este caso la tarea de la red neuronal es predecir una palabra dada su contexto.\n",
    "\n",
    "En este caso, para el cálculo de la verosimilitud de una palabra   $w_n$ se considera:\n",
    "\n",
    "$$w_n \\sim softmax(\\rho^T \\alpha_{d_n})$$\n",
    "\n",
    "donde $\\rho \\in M_{LxN}$ es la matriz que contiene en sus columnas el vector de embedding  de cada palabra del vocabulario y $\\alpha_{d_n} \\in R^L$ es el vector de contexto correspondiente a la suma de los vectores de embedding de las palabras en el contexto de la palabra $w_n$.\n",
    "\n",
    "**Skip-gram (SG)** La red neuronal asociada tiene como tarea predecir las palabras del contexto de una palabra dada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"modelos_embedding.png\" width=600 height=500>         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skip-gram.png\" width=600 height=500>\n",
    "\n",
    "*imagen proveniente de Tesis de Magíster de Pablo Badilla, U. de Chile*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones objetivos mas utilizadas para optimizar estas redes neuronales artificiales son:\n",
    "\n",
    "- Softmax jerárquico\n",
    "\n",
    "- Negative sampling \n",
    "\n",
    "La biblioteca *word2vec* implementa CBOW y SG\n",
    "\n",
    "Otras arquitecturas de red y funciones objetivo definen otros métodos de word embedding:\n",
    "\n",
    "- Glove (global vectors): considera el contexto global de cada palabra en la función de optimización, no sólo el contexto local\n",
    "\n",
    "\n",
    "- Lexvec (Lexical vectors): incorpora la medida PPMI (positive point-wise mutual information) en la optimización de la arquitectura skip-gram.\n",
    "\n",
    "\n",
    "- FastText: incorpora los n-gramas que componen una palabra.\n",
    "\n",
    "\n",
    "- ConceptNet: considera la elaboración de un grafo de conocimiento (en base a diversas fuentes) con las palabras del vocabulario que se utiliza para el ajuste de la red neuronal.\n",
    "\n",
    "\n",
    "**Limitaciones de modelos de word embedding*:\n",
    "\n",
    "- Las redes pre-entrenadas para generar la representación vectorial dependen fuertemente de los corpus utilizados.\n",
    "\n",
    "- Mayormente desarrollados en inglés. Algunas iniciativas en español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Word Embedding (AWE)**\n",
    "\n",
    "Incorpora mejoras en el proceso de optimización en CBOW de manera que el vector de contexto que se utiliza para definir la función objetivo no le de la misma importancia a todas las palabras del contexto, ajustando pesos distintos de las palabras en el contexto de la palabra a predecir mediante mecanismos de atención (matrices key y query).\n",
    "\n",
    "Existe también una versión AWE-S que considera cada palabra descompuesta en n-gramas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Topic Models (ETM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se propone el siguiente proceso generativo secuencial:\n",
    "\n",
    "\n",
    "1. Por cada documento generar la distribución de tópicos de una **Logistic Normal**:\n",
    "\n",
    "$$\\theta_d \\sim {\\cal{LN}}(0,I_{KxK})$$\n",
    "    \n",
    "2. Por cada documento $d$ generar la palabra $n$ de la manera siguiente:\n",
    "\n",
    "    a. Asignar un tópico:\n",
    "    \n",
    "    $$z_{d,n} \\sim Mult(\\theta_d)$$\n",
    "    \n",
    "    b. Generar la palabra:\n",
    "    \n",
    "    $$w_{d,n} \\sim Mult(softmax(\\rho^T \\alpha_{z_{d,n}}))$$\n",
    "    \n",
    "donde $\\rho \\in M_{LxN}$ es la matriz que contiene en sus columnas el vector de embedding  de cada palabra del vocabulario y $\\alpha_{z_{d_n}} \\in R^L$ es el vector de embedding del tópico $z_{d,n}$.\n",
    "        \n",
    "<img src=\"etm.png\" width=600 height=200>            \n",
    "\n",
    "Como en los modelos anteriores, las distribuciones a posteriori no tienen expresiones analíticas, por lo que se recurre a la Inferencia Variacional para aproximarlas.\n",
    "\n",
    "Los resultados de ETM  mejoran sustancialmente el desempeño de LDA al aumentar el tamaño del vocabulario.\n",
    "\n",
    "\n",
    "<img src=\"etm_vs-lda.png\" width=400 height=400>       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Embedding Topic Models (D-ETM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, el proceso generativo secuencial se define como sigue:\n",
    "\n",
    "1. Generar vectores de embedding iniciales para cada tópico:\n",
    "\n",
    "$$\\alpha_{k}^0 \\sim {\\cal N} (0, I_{LxL})$$\n",
    "\n",
    "\n",
    "2. Generar proporción media de tópicos inicial:\n",
    "\n",
    "$$\\eta_0 \\sim {\\cal N} (0, I_{KxK})$$\n",
    "\n",
    "\n",
    "3. Por cada franja de tiempo $t= 1, \\cdots, T$:\n",
    "\n",
    "    a. Generar vectores de embedding por cada tópico $ k=1,\\cdots,K$: \n",
    "    \n",
    "    $$\\alpha_{k}^{(t)} \\sim {\\cal N} (\\alpha_{k}^{(t-1)}, \\gamma^2 I_{LxL})$$\n",
    "        \n",
    "    b. Generar proporción media de tópicos:\n",
    "\n",
    "    $$\\eta_t \\sim {\\cal N} (\\eta_{t-1}, \\delta^2 I_{KxK})$$\n",
    "\n",
    "4. Por cada documento:\n",
    "\n",
    "    a.Generar la distribución de tópicos de una **Logistic Normal**:\n",
    "\n",
    "    $$\\theta_d \\sim {\\cal{LN}}(\\eta_{t_d},I_{KxK})$$\n",
    "    \n",
    "    b. Generar la palabra $n$ de la manera siguiente:\n",
    "\n",
    "      i. Asignar un tópico:\n",
    "    \n",
    "    $$z_{d,n} \\sim Mult(\\theta_d)$$\n",
    "    \n",
    "      ii. Generar la palabra:\n",
    "    \n",
    "    $$w_{d,n} \\sim Mult \\left(softmax(\\rho^T \\alpha_{z_{d,n}}^{(t_d)})\\right)$$\n",
    "    \n",
    "donde $\\rho \\in M_{LxN}$ es la matriz que contiene en sus columnas el vector de embedding  de cada palabra del vocabulario y $\\alpha_{z_{d_n}}^{(t_d)} \\in R^L$ es el vector de embedding del tópico $z_{d,n}$ en la franja de tiempo $t_d$.\n",
    "        \n",
    "        \n",
    "<img src=\"detm.png\" width=400 height=300>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La inferencia en este caso es nuevamente por aproximación numérica del problema de optimización, es decir inferencia variacional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplos**\n",
    "<img src=\"ejemplo2_detm.png\" width=800 height=200>    \n",
    "<img src=\"ejemplos_detm.png\" width=800 height=300>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de D-ETM mejoran en la mayor parte de los casos el desempeño de D-LDA y D-LDA-REP:\n",
    "\n",
    "<img src=\"ejemplo3_detm.png\" width=600 height=200>\n",
    "\n",
    "<img src=\"ejemplo4_detm.png\" width=600 height=200>\n",
    "\n",
    "\n",
    "Topic Coherence (TC): considerando las top N palabras de cada tópico.\n",
    "\n",
    "Topic Diversity (TD):  es el porcentaje de palabras únicas en las top 25 palabras de cada tópico.\n",
    "\n",
    "Topic Quality (TQ): es el producto de TC y TD.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variaciones de D-ETM\n",
    "\n",
    "Modelos propuestos en *Global Surveillance of COVID-19 by mining news media using a multi-source dynamic embedded topic model*,Li et al, BCB 2020, https://dl.acm.org/doi/10.1145/3388440.3412418\n",
    "\n",
    "<img src=\"modelo_Li.png\" width=800 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ejemplo_nuevo.png\" width=800 height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ejemplo_clases.png\" width=400 height=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
